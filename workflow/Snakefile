configfile: "config/config_bin.yaml"

import pandas as pd
df_samples = pd.read_csv(config["SAMPLES"], sep ='\t', index_col = 0)
samples = df_samples["sample"].to_list()

JOBID = config["JOBID"]
REFIN = config["REFIN"]
RAW_DIR = config["RAW_DIR"]
FILE_EX = config["FILE_EX"]
MAPPING = config["MAPPING"]
CONTIG_T = config["CONTIG_T"]
P_THRESH = config["P_THRESH"]
COUNT_METHOD = config["COUNT_METHOD"]


rule all:
    input:
        expand("{REFIN}.sa", REFIN=REFIN),
        expand("results/clustering/{JOBID}_contig_lengths.tsv", JOBID = JOBID),
        expand('results/alignment/{SAMPLES}.bam', SAMPLES = samples),
        expand('results/alignment/{SAMPLES}_read_len.tsv', SAMPLES = samples),
        expand('results/clustering/{SAMPLES}_counts.tsv', SAMPLES = samples),
        expand('results/clustering/{SAMPLES}_counts.tsv', SAMPLES=samples),
        expand('results/clustering/{SAMPLES}_proc_counts_out.tsv', SAMPLES = samples),
        expand('results/clustering/{JOBID}_merged_counts.tsv', JOBID= JOBID),
        expand('results/clustering/{jobid}_read_counts_derived.csv', jobid= JOBID),
        expand('results/clustering/{jobid}_values.csv', jobid = JOBID),
        expand('results/clustering/{jobid}_diffs.csv', jobid = JOBID),
        expand("results/clustering/{JOBID}_bwa_output.txt", JOBID = JOBID)

localrules: split_file

rule bwa_index:
    input:
        ref = REFIN
    output:
        '{REFIN}.sa'
    threads: 20
    conda: "workflow/envs/bwasam.yaml"
    shell:
        """
        bwa index {input.ref}
        """

if MAPPING == "SR":
  rule bwa_mem:
      input:
          fq = expand('{RAW_READ}{{samples}}.{extension}',  RAW_READ=RAW_DIR, extension = FILE_EX),
          ref = REFIN,
          ref_ind = expand("{reference}.sa", reference=REFIN) #waits for indexed reference
      output:
          bam = 'results/alignment/{samples}.bam',
          read_len = 'results/alignment/{samples}_read_len.tsv'
      conda: "workflow/envs/bwasam.yaml"
      threads: 6
      shell:
          """
          bwa mem -M -t {threads} {input.ref} {input.fq} | samtools view -buS - | samtools sort -o {output.bam}
          bioawk -c fastx '{{ print $name, length($seq) }}' < {input.fq} > {output.read_len}
          """

elif MAPPING == "paired_SR":
  rule bwa_mem_pair:
      input:
          fq1 = expand('{RAW_READ}{{samples}}_R1.{extension}', RAW_READ=RAW_DIR, extension=FILE_EX),
          fq2 = expand('{RAW_READ}{{samples}}_R2.{extension}',  RAW_READ=RAW_DIR, extension=FILE_EX),
          ref = REFIN,
          ref_ind = expand("{reference}.sa", reference=REFIN) #waits for indexed reference
      output:
          bam = 'results/alignment/{samples}.bam',
          read_len = 'results/alignment/{samples}_read_len.tsv'
      conda: "workflow/envs/bwasam.yaml"
      threads: 6
      shell:
          """
          bwa mem -M -t {threads} {input.ref} {input.fq1} {input.fq2} | samtools view -buS - | samtools sort -o {output.bam}
          bioawk -c fastx '{{ print $name, length($seq) }}' < {input.fq1} > {output.read_len}
          """

elif MAPPING == "LR":
    rule minimap:
        input:
          fq = expand('{RAW_READ}{{samples}}.{extension}',  RAW_READ=RAW_DIR, extension=FILE_EX),
          ref = REFIN,
          ref_ind = expand("{reference}.sa", reference=REFIN) #waits for indexed reference
        output:
            bam = 'results/alignment/{samples}.bam',
            read_len = 'results/alignment/{samples}_read_len.tsv'
        params:
            output = 'results/alignment/{samples}.sam'
        conda: "workflow/envs/minimap2.yaml"
        threads: 6
        shell:
            """
            minimap2 -ax map-ont -o {params.output} -t {threads} {input.ref} {input.fq}
            samtools view -buS {params.output} | samtools sort -o {output.bam}
            bioawk -c fastx '{{ print $name, length($seq) }}' < {input.fq} > {output.read_len}
            """

rule count_files:
    input:
        bam = 'results/alignment/{samples}.bam'
    output:
        counts = 'results/clustering/{samples}_counts.tsv',
    params:
        map = MAPPING,
    conda: "workflow/envs/minimap2.yaml"
    shell:
        """
        samtools view {input.bam} | awk '{{if ($2!=4 && $2!=133 && $2!=165 && $2!=181 && $2!=101 && $2!=117 && $2!=69 && $2!=77 && $2!=141) print $1 "\t" $3}}' > {output.counts}
        """


rule contig_len:
    input:
        ref = REFIN
    output:
        contig_lengths = "results/clustering/{JOBID}_contig_lengths.tsv"
    conda: "workflow/envs/minimap2.yaml"
    shell:
      """
      bioawk -c fastx '{{ print $name, length($seq) }}' < {input.ref} > {output.contig_lengths}
      """


rule norm_counts:
      input:
          counts = 'results/clustering/{samples}_counts.tsv',
          read_len = 'results/alignment/{samples}_read_len.tsv',
          contig_list = expand("results/clustering/{JOBID}_contig_lengths.tsv", JOBID = JOBID)
      output:
          norm_count = 'results/clustering/{samples}_proc_counts_out.tsv'
      conda: "workflow/envs/py3.yaml"
      shell:
        """
        python scripts/python/norm_counts.py {input.counts} {input.read_len} {input.contig_list} {output.norm_count}
        """

rule merge_filecounts:
    input:
        test = expand('results/clustering/{SAMPLES}_proc_counts_out.tsv', SAMPLES = samples)
    output:
        tsv = 'results/clustering/{JOBID}_merged_counts.tsv'
    params:
        method = COUNT_METHOD,
        ref = REFIN
    conda: "workflow/envs/py3.yaml"
    shell:
        """
        python scripts/python/merge_filecounts.py {params.method} {output.tsv}  -l {input.test}
        """

rule derive:
    input:
        expand('results/clustering/{JOBID}_merged_counts.tsv', JOBID=JOBID)
    output:
        csv = "results/clustering/{JOBID}_read_counts_derived.csv"
    params:
        thresh = CONTIG_T
    conda:
        "workflow/envs/py3.yaml"
    shell:
        """
        python scripts/python/gen_counts_csv.py {input} {JOBID} {params.thresh}
        """

rule start_feeder:
    input:
        expand('results/clustering/{JOBID}_read_counts_derived.csv', JOBID=JOBID, method = COUNT_METHOD)
    output:
        values = "results/clustering/{JOBID}_values.csv",
        diffs = "results/clustering/{JOBID}_diffs.csv"
    conda:
        "workflow/envs/py3.yaml"
    shell:
        """
        python scripts/python/start_feeder.py clustering {JOBID}
        """

rule split_file:
    input:
        diffs = expand("results/clustering/{JOBID}_diffs.csv", JOBID=JOBID)
    output:
        touch("results/clustering/{JOBID}_bwa_output.txt")
    params:
        diffs = expand("results/clustering/{JOBID}_diffs", JOBID = JOBID)
    shell:
        """
        split -d -l 10000 --additional-suffix=.csv {input.diffs} {params.diffs}
        """
