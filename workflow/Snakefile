
configfile: "config/config_analysis.yaml"


JOBID = config["JOBID"] # this can be different from binning pipeline
REFIN = config["REFIN"]
BIN_LOC = config["BIN_LOC"]
kraken_level = config["kraken_level"]
krakendb = config["krakendb"]
checkmdb = config["checkm_db_root"]
GTDB = config["GTDB"]

gtdb_v_tar_bac = "https://data.ace.uq.edu.au/public/gtdb/data/releases/release95/95.0/bac120_metadata_r95.tar.gz"
gtdb_v_tar_arc = "https://data.ace.uq.edu.au/public/gtdb/data/releases/release95/95.0/ar122_metadata_r95.tar.gz"
gtdb_v_bac = "bac120_metadata_r95.tsv"
gtdb_v_arc = "ar122_metadata_r95.tsv"


if BIN_LOC.endswith("/") == False :
    BIN_LOC = BIN_LOC + "/"

# FILE_EX = config["FILE_EX"]
# MAPPING = config["MAPPING"]
# CONTIG_T = config["CONTIG_T"]
# P_THRESH = config["P_THRESH"]
# COUNT_METHOD = config["COUNT_METHOD"]

location = BIN_LOC + "Cluster_{CLUSTER}.fasta"

(CLUSTERS,) = glob_wildcards(location)

rule all:
    input:
        expand("analysis/prokka/{JOBID}/Cluster_{CLUSTERS}/Cluster_{CLUSTERS}.{ext}", JOBID = JOBID, CLUSTERS = CLUSTERS, ext=[ "err", "fna", "faa", "gff", "gbk", "ffn", "sqn", "fsa", "tbl", "log", "txt", "tsv"]),
        expand("analysis/kraken/{JOBID}/{JOBID}_Cluster_{CLUSTERS}_report_kraken.out", JOBID = JOBID, CLUSTERS = CLUSTERS),
        expand("analysis/kraken/{JOBID}/{JOBID}_{kraken_level}_top_kraken.out", JOBID = JOBID, kraken_level = kraken_level),
        expand("analysis/kraken/{JOBID}/{JOBID}_{kraken_level}_GTDB_lookup.json", JOBID=JOBID, kraken_level = kraken_level),
        expand("analysis/{JOBID}_seqkit_stats.tsv", JOBID=JOBID),
        expand("analysis/checkm/{JOBID}/{JOBID}_checkm.log", JOBID=JOBID)


localrules: kraken_merge#, output

rule prokka:
    input:
        clusters = expand("{BIN_LOC}Cluster_{{CLUSTERS}}.fasta", BIN_LOC=BIN_LOC)
    output:
        file = expand("analysis/prokka/{{JOBID}}/Cluster_{{CLUSTERS}}/Cluster_{{CLUSTERS}}.{ext}", ext=[ "err", "fna", "faa", "gff", "gbk", "ffn", "sqn", "fsa", "tbl", "log", "txt", "tsv"])
    params:
        dir = "analysis/prokka/{JOBID}/Cluster_{CLUSTERS}/",
        prefix = "Cluster_{CLUSTERS}",
        prokka = "results/clusters/Cluster_{CLUSTERS}_short.fasta"
    conda:
        "envs/prokka.yaml"
    threads:
        20
    shell:
        """
        awk '/^>/{{print substr($1,1,21); next}}{{print}}' < {input.clusters} > {params.prokka}
        prokka {params.prokka} --outdir {params.dir} --prefix {params.prefix} --cpus {threads} --force
        rm {params.prokka}
        """

rule kraken:
    input:
        expand("{BIN_LOC}Cluster_{{CLUSTERS}}.fasta", BIN_LOC=BIN_LOC)
    output:
        report = "analysis/kraken/{JOBID}/{JOBID}_Cluster_{CLUSTERS}_report_kraken.out",
    params:
        db = krakendb,
        output = "analysis/kraken/{JOBID}/{JOBID}_Cluster_{CLUSTERS}_kraken.out"
    conda:
        "envs/kraken2.yaml"
    threads:
        16
    resources:
        mem_mb = 4000
    shell:
        """
        kraken2 -db {params.db} --threads {threads} --report {output.report} --output {params.output} --use-names {input}
        """

rule kraken_merge:
    input:
        report = expand("analysis/kraken/{JOBID}/{JOBID}_Cluster_{CLUSTERS}_report_kraken.out", JOBID = JOBID, CLUSTERS = CLUSTERS)
    output:
        "analysis/kraken/{JOBID}/{JOBID}_{kraken_level}_top_kraken.out"
    params:
        level = {kraken_level}
    shell:
        """
        cd analysis/kraken/{JOBID}
        find -name '{JOBID}*_report_kraken.out' -type f -printf '\\n%p\\t' -exec sh -c 'echo {{}} | sort -k1nr {{}} | grep -P "\\t{params.level}\\t" | head -n1 ' \\; > {JOBID}_{params.level}_top_kraken.out
        """

if GTDB == "Y":
    rule GTDB_download:
      input:
        kraken_top = expand("analysis/kraken/{JOBID}/{JOBID}_{kraken_level}_top_kraken.out", JOBID = JOBID, kraken_level = kraken_level)
      output:
        bac120_gtdb = temp(expand("tmp/gtdb/{gtdb_v_bac}", gtdb_v_bac = gtdb_v_bac)),
        ar122_gtdb = temp(expand("tmp/gtdb/{gtdb_v_arc}", gtdb_v_arc = gtdb_v_arc))
      params:
        loc = "tmp/gtdb/",
        link_bac = gtdb_v_tar_bac,
        link_arc = gtdb_v_tar_arc
      shell:
        """
        if wget -q --spider {params.link_bac}; then
          wget -O {params.loc}bac120_metadata.tar.gz {params.link_bac}
          tar -xf {params.loc}bac120_metadata.tar.gz -C {params.loc}
        fi

        if wget -q --spider {params.link_arc}; then
          wget -O {params.loc}ar122_metadata.tar.gz {params.link_arc}
          tar -xf {params.loc}ar122_metadata.tar.gz -C {params.loc}
        fi
        """

    rule gtdb_lookup:
      input:
        bac120_gtdb = expand("tmp/gtdb/{gtdb_v_bac}", gtdb_v_bac = gtdb_v_bac),
        ar122_gtdb = expand("tmp/gtdb/{gtdb_v_arc}", gtdb_v_arc = gtdb_v_arc),
        kraken_top = expand("analysis/kraken/{JOBID}/{JOBID}_{kraken_level}_top_kraken.out", JOBID = JOBID, kraken_level = kraken_level)
      output:
        json_lookup = "analysis/kraken/{JOBID}/{JOBID}_{kraken_level}_GTDB_lookup.json"
      params:
        level = {kraken_level}
      conda:
            "envs/py3.yaml"
      shell:
        """
        python workflow/scripts/python/gtdb_lookup.py {input.kraken_top} {params.level} {input.bac120_gtdb} {input.ar122_gtdb} {output.json_lookup}
        """

elif GTDB == "N":
    rule empty_lookup:
      input:
        kraken_top = expand("analysis/kraken/{JOBID}/{JOBID}_{kraken_level}_top_kraken.out", JOBID = JOBID, kraken_level = kraken_level)
      output:
        "analysis/kraken/{JOBID}/{JOBID}_{kraken_level}_GTDB_lookup.json"
      shell:
        """
        touch {output}
        """

rule seqkit:
  input:
      wait_for = expand("analysis/kraken/{JOBID}/{JOBID}_{kraken_level}_GTDB_lookup.json", JOBID = JOBID, kraken_level = kraken_level),
  output:
      "analysis/{JOBID}_seqkit_stats.tsv"
  params:
      binning_loc = BIN_LOC
  conda:
      "envs/py3.yaml"
  threads:
      10
  shell:
      """
      seqkit stats -a -T -j {threads} {params.binning_loc}*.fasta > {output}
      """

rule checkm:
  input:
      expand("results/clusters/Cluster_{CLUSTERS}.fasta", CLUSTERS = CLUSTERS)
  output:
      expand("analysis/checkm/{JOBID}/{JOBID}_checkm.log", JOBID=JOBID)
  params:
      out = expand("analysis/checkm/{JOBID}", JOBID=JOBID),
      input = BIN_LOC,
      refin = REFIN,
      checkmdb = checkmdb
  threads:
      20
  conda:
      "envs/checkm.yaml"
  shell:
      """
      checkm_db={params.checkmdb}
      echo ${{checkm_db}} | checkm data setRoot ${{checkm_db}}
      checkm unbinned -x fasta {params.input} {params.refin} analysis/{JOBID}_unbinned_contigs.fa analysis/{JOBID}_unbinned_contigs_stats.tsv
      checkm lineage_wf -f {output} --tab_table -x fasta -t {threads} {params.input} {params.out}
      """
