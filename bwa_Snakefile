samples= 'NAB1_T0 NAB1_T1 NAB1_T2 NAB1_T3 NAB1_T4 NAB1_T5 NAB1_T6 NAB1_T7 NAB1_T8 NAB1_T9 NAB1_T10 NAB1_T11 NAB1_T12 NAB1_T13 NAB1_T14 NAB1_T15 NAB1_T16 NAB1_T17 NAB2_T0 NAB2_T1 NAB2_T2 NAB2_T3 NAB2_T4 NAB2_T5 NAB2_T6 NAB2_T7 NAB2_T8 NAB2_T9 NAB2_T10 NAB2_T11 NAB2_T12 NAB2_T13 NAB2_T14 NAB2_T15 NAB2_T16 NAB2_T17 NAB3_T0 NAB3_T1 NAB3_T2 NAB3_T3 NAB3_T4 NAB3_T5 NAB3_T6 NAB3_T7 NAB3_T8 NAB3_T9 NAB3_T10 NAB3_T11 NAB3_T12 NAB3_T13 NAB3_T14 NAB3_T15 NAB3_T16 NAB3_T17 NAB4_T0 NAB4_T1 NAB4_T2 NAB4_T3 NAB4_T4 NAB4_T5 NAB4_T6 NAB4_T7 NAB4_T8 NAB4_T9 NAB4_T10 NAB4_T11 NAB4_T12 NAB4_T13 NAB4_T14 NAB4_T15 NAB4_T16 NAB4_T17 NAB_Feed_T9 NAB_Feed_T10 NAB_Feed_T11 NAB_Feed_T12 NAB_Feed_T13 NAB_Feed_T14 NAB_Feed_T15 NAB_Feed_T17' #should be in order want output
JOBID = 'NAB_all'
RAW_SR = 'data/'
REFIN = 'data/yw_polished_anvio.fasta'
THRESH = '1000'
P_THRESH = '0.99'

#run using snakemake --cluster "sbatch -t 02:00:00" -j 20

rule all:
    input:
        expand("{REFIN}.sa", REFIN=REFIN),
        expand('inter/counts_{samples}.txt', samples=samples.split(' ')),
        expand('inter/{jobid}_read_counts.out', jobid= JOBID),
        expand('inter/{jobid}_read_counts_derived.csv', jobid= JOBID),
        expand('inter/{jobid}_values.csv', jobid = JOBID),
        expand('inter/{jobid}_diffs.csv', jobid = JOBID),
        expand("inter/{JOBID}_output.txt", JOBID = JOBID),

rule bwa_index:
    input:
        ref = REFIN
    output:
        '{REFIN}.sa'
    shell:
        'bwa index {input.ref}'

rule bwa_mem:
    input:
        fq1 = 'data/{samples}_R1.fastq.gz',
        fq2 = 'data/{samples}_R2.fastq.gz',
        ref = REFIN,
        ref_ind = expand("{reference}.sa", reference=REFIN)
    output:
        counts = 'inter/counts_{samples}.txt'
    params:
        bam = 'bwa_out/{samples}.bam'
    threads: 16
    shell:
        """
        mkdir -p bwa_out
        bwa mem -M -t {threads} {input.ref} {input.fq1} {input.fq2} | samtools view -bhS - | samtools sort -o {params.bam}
        samtools index {params.bam}
        samtools idxstats {params.bam} > {output.counts}
        """

rule merge_filecounts:
    input:
        test = expand('inter/counts_{SAMPLES}.txt', SAMPLES = samples.split(' '))
    output:
        txt = 'inter/{JOBID}_read_counts.out'
    conda:
        "envs/py3.yaml"
    shell:
        """
        python scripts/merge_filecounts.py inter {JOBID}
        """

rule derive:
    input:
        expand('inter/{JOBID}_read_counts.out', JOBID=JOBID)
    output:
        csv = "inter/{JOBID}_read_counts_derived.csv"
    params:
        thresh = THRESH
    conda:
        "envs/py3.yaml"
    shell:
        """
        python scripts/derive.py inter {JOBID} {params.thresh}
        """

rule start_feeder:
    input:
        expand('inter/{JOBID}_read_counts_derived.csv', JOBID=JOBID)
    output:
        values = "inter/{JOBID}_values.csv",
        diffs = "inter/{JOBID}_diffs.csv"
    conda:
        "envs/py3.yaml"
    shell:
        """
        python scripts/start_feeder.py inter {JOBID}
        """

rule split_file:
    input:
        diffs = expand("inter/{JOBID}_diffs.csv", JOBID=JOBID)
    output:
        expand("inter/{JOBID}_output.txt", JOBID = JOBID)
    params:
        diffs = expand("inter/{JOBID}_diffs", JOBID = JOBID)
    shell:
        """
        split -d -l 10000 --additional-suffix=.csv {input.diffs} {params.diffs}
        echo "Done" > {output}
        """
