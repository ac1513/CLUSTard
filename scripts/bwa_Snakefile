configfile: "config.yaml"

import pandas as pd
df_samples = pd.read_csv(config["samples"], sep ='\t', index_col = 0)
samples = df_samples["sample"].to_list()

JOBID = config["jobid"]
RAW_SR = config["RAW_SR"]
REFIN = config["REFIN"]
CONTIG_T = config["CONTIG_T"]
P_THRESH = config["P_THRESH"]
krakendb = config["krakendb"]
kraken_level = config["kraken_level"]


rule all:
    input:
        expand("{REFIN}.sa", REFIN=REFIN),
        expand('output/clustering/counts_{samples}.txt', samples=samples),
        expand('output/clustering/{jobid}_read_counts.out', jobid= JOBID),
        expand('output/clustering/{jobid}_read_counts_derived.csv', jobid= JOBID),
        expand('output/clustering/{jobid}_values.csv', jobid = JOBID),
        expand('output/clustering/{jobid}_diffs.csv', jobid = JOBID),
        expand("output/clustering/{JOBID}_bwa_output.txt", JOBID = JOBID),

rule bwa_index:
    input:
        ref = REFIN
    output:
        '{REFIN}.sa'
    benchmark:
        "benchmarks/bwa_in.tsv"
    shell:
        """
        module load bio/BWA
        bwa index {input.ref}
        """

rule bwa_mem:
    input:
        fq1 = 'data/{samples}_R1.fastq.gz',
        fq2 = 'data/{samples}_R2.fastq.gz',
        ref = REFIN,
        ref_ind = expand("{reference}.sa", reference=REFIN) #waits for indexed reference
    output:
        counts = 'output/clustering/counts_{samples}.txt'
    params:
        bam = 'output/alignment/{samples}.bam'
    benchmark:
        "benchmarks/bwa.tsv"
    threads: 16
    shell:
        """
        module load bio/BWA
        module load bio/SAMtools
        mkdir -p output/alignment
        bwa mem -M -t {threads} {input.ref} {input.fq1} {input.fq2} | samtools view -buS - | samtools sort -o {params.bam}
        samtools index {params.bam}
        samtools idxstats {params.bam} > {output.counts}
        """

rule merge_filecounts:
    input:
        test = expand('output/clustering/counts_{SAMPLES}.txt', SAMPLES = samples)
    output:
        txt = 'output/clustering/{JOBID}_read_counts.out'
    conda:
        "../envs/py3.yaml"
    benchmark:
        "benchmarks/merge_files.tsv"
    shell:
        """
        python scripts/merge_filecounts.py clustering {JOBID}
        """

rule derive:
    input:
        expand('output/clustering/{JOBID}_read_counts.out', JOBID=JOBID)
    output:
        csv = "output/clustering/{JOBID}_read_counts_derived.csv"
    params:
        thresh = CONTIG_T
    benchmark:
        "benchmarks/derive.tsv"
    conda:
        "../envs/py3.yaml"
    shell:
        """
        python scripts/derive.py clustering {JOBID} {params.thresh}
        """

rule start_feeder:
    input:
        expand('output/clustering/{JOBID}_read_counts_derived.csv', JOBID=JOBID)
    output:
        values = "output/clustering/{JOBID}_values.csv",
        diffs = "output/clustering/{JOBID}_diffs.csv"
    conda:
        "../envs/py3.yaml"
    benchmark:
        "benchmarks/start_feeder.tsv"
    shell:
        """
        python scripts/start_feeder.py clustering {JOBID}
        """

rule split_file:
    input:
        diffs = expand("output/clustering/{JOBID}_diffs.csv", JOBID=JOBID)
    output:
        touch("output/clustering/{JOBID}_bwa_output.txt")
    params:
        diffs = expand("output/clustering/{JOBID}_diffs", JOBID = JOBID)
    shell:
        """
        split -d -l 10000 --additional-suffix=.csv {input.diffs} {params.diffs}
        """
