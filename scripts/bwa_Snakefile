configfile: "config.yaml"

import pandas as pd
df_samples = pd.read_csv(config["samples"], sep ='\t', index_col = 0)
samples = df_samples["sample"].to_list()

JOBID = config["jobid"]
RAW_SR = config["RAW_SR"]
REFIN = config["REFIN"]
CONTIG_T = config["CONTIG_T"]
P_THRESH = config["P_THRESH"]
krakendb = config["krakendb"]
kraken_level = config["kraken_level"]
extension = config["in_file_extension"].strip('.')
mapping = "LR"
meth_counts = "norm"

rule all:
    input:
        expand("{REFIN}.sa", REFIN=REFIN),
        expand("output/clustering/{JOBID}_contig_lengths.tsv", JOBID = JOBID),
        expand('output/alignment/{SAMPLES}.bam', SAMPLES = samples),
        expand('output/clustering/{SAMPLES}_read_len.tsv', SAMPLES = samples),
        expand('output/clustering/{SAMPLES}_counts.tsv', SAMPLES = samples),
        expand('output/clustering/{SAMPLES}_counts.tsv', SAMPLES=samples),
        expand('output/clustering/{SAMPLES}_proc_counts_out.tsv', SAMPLES = samples),
        expand('output/clustering/{JOBID}_merged_counts.tsv', JOBID= JOBID),
        expand('output/clustering/{jobid}_read_counts_derived.csv', jobid= JOBID),
        expand('output/clustering/{jobid}_values.csv', jobid = JOBID),
        expand('output/clustering/{jobid}_diffs.csv', jobid = JOBID),
        expand("output/clustering/{JOBID}_bwa_output.txt", JOBID = JOBID)


localrules: contig_list, norm_counts, merge_filecounts, derive, start_feeder, split_file


rule bwa_index:
    input:
        ref = REFIN
    output:
        '{REFIN}.sa'
    threads: 20
    conda: "../envs/bwasam.yaml"
    shell:
        """
        bwa index {input.ref}
        """

if mapping == "SR":
  rule bwa_mem:
      input:
          fq = expand('data/{{samples}}.{extension}', extension = extension),
          ref = REFIN,
          ref_ind = expand("{reference}.sa", reference=REFIN) #waits for indexed reference
      output:
          bam = 'output/alignment/{samples}.bam',
          read_len = 'output/clustering/{samples}_read_len.tsv'
      conda: "../envs/bwasam.yaml"
      threads: 6
      shell:
          """
          bwa mem -M -t {threads} {input.ref} {input.fq} | samtools view -buS - | samtools sort -o {output.bam}
          bioawk -c fastx '{{ print $name, length($seq) }}' < {input.fq} > {output.read_len}
          """

elif mapping == "paired_SR":
  rule bwa_mem_pair:
      input:
          fq1 = expand('data/{{samples}}_R1.{extension}', extension=extension),
          fq2 = expand('data/{{samples}}_R2.{extension}', extension=extension),
          ref = REFIN,
          ref_ind = expand("{reference}.sa", reference=REFIN) #waits for indexed reference
      output:
          bam = 'output/alignment/{samples}.bam',
          read_len = 'output/clustering/{samples}_read_len.tsv'
      conda: "../envs/bwasam.yaml"
      threads: 6
      shell:
          """
          bwa mem -M -t {threads} {input.ref} {input.fq1} {input.fq2} | samtools view -buS - | samtools sort -o {output.bam}
          bioawk -c fastx '{{ print $name, length($seq) }}' < {input.fq1} > {output.read_len}
          """

elif mapping == "LR":
    rule minimap:
        input:
          fq = expand('data/{{samples}}.{extension}', extension=extension),
          ref = REFIN,
          ref_ind = expand("{reference}.sa", reference=REFIN) #waits for indexed reference
        output:
            bam = 'output/alignment/{samples}.bam',
            read_len = 'output/clustering/{samples}_read_len.tsv'
        conda: "../envs/minimap2.yaml"
        threads: 6
        shell:
            """
            minimap2 -ax map-ont -o {samples}.sam -t {threads} {input.ref} {input.fq}
            samtools view -buS {samples}.sam | samtools sort -o {output.bam}
            bioawk -c fastx '{{ print $name, length($seq) }}' < {input.fq} > {output.read_len}
            """

rule count_files:
    input:
        bam = 'output/alignment/{samples}.bam'
    output:
        counts = 'output/clustering/{samples}_counts.tsv',
    params:
        map = mapping,
    conda: "../envs/py3.yaml"
    shell:
        """
        samtools view {input.bam} | awk '{{if ($2!=4 && $2!=133 && $2!=165 && $2!=181 && $2!=101 && $2!=117 && $2!=69 && $2!=77 && $2!=141) print $1 "\t" $3}}' > {output.counts}
        """

rule norm_counts:
      input:
          counts = 'output/clustering/{samples}_counts.tsv',
          read_len = 'output/clustering/{samples}_read_len.tsv',
          contig_list = expand("output/clustering/{JOBID}_contig_lengths.tsv", JOBID = JOBID)
      output:
          norm_count = 'output/clustering/{samples}_proc_counts_out.tsv'
      conda: "../envs/py3.yaml"
      shell:
        """
        python scripts/norm_counts.py {input.counts} {input.read_len} {input.contig_list} {output.norm_count}
        """

rule merge_filecounts:
    input:
        test = expand('output/clustering/{SAMPLES}_proc_counts_out.tsv', SAMPLES = samples)
    output:
        tsv = 'output/clustering/{JOBID}_merged_counts.tsv'
    params:
        method = meth_counts,
        ref = REFIN
    conda:
        "../envs/py3.yaml"
    shell:
        """
        python scripts/merge_filecounts2.py {params.method} {output.tsv}  -l {input.test}
        """

rule derive:
    input:
        expand('output/clustering/{JOBID}_merged_counts.tsv', JOBID=JOBID)
    output:
        csv = "output/clustering/{JOBID}_read_counts_derived.csv"
    params:
        thresh = CONTIG_T
    conda:
        "../envs/py3.yaml" #change clustering (below) when add counts folder..
    shell:
        """
        python scripts/derive.py {input} {JOBID} {params.thresh}
        """

rule start_feeder:
    input:
        expand('output/clustering/{JOBID}_read_counts_derived.csv', JOBID=JOBID, method = meth_counts)
    output:
        values = "output/clustering/{JOBID}_values.csv",
        diffs = "output/clustering/{JOBID}_diffs.csv"
    conda:
        "../envs/py3.yaml"
    shell:
        """
        python scripts/start_feeder.py clustering {JOBID}
        """

rule split_file:
    input:
        diffs = expand("output/clustering/{JOBID}_diffs.csv", JOBID=JOBID)
    output:
        touch("output/clustering/{JOBID}_bwa_output.txt")
    params:
        diffs = expand("output/clustering/{JOBID}_diffs", JOBID = JOBID)
    shell:
        """
        split -d -l 10000 --additional-suffix=.csv {input.diffs} {params.diffs}
        """
