configfile: "config.yaml"

import pandas as pd
df_samples = pd.read_csv(config["samples"], sep ='\t', index_col = 0)
samples = df_samples["sample"].to_list()

JOBID = config["jobid"]
RAW_SR = config["RAW_SR"]
REFIN = config["REFIN"]
CONTIG_T = config["CONTIG_T"]
P_THRESH = config["P_THRESH"]
krakendb = config["krakendb"]
kraken_level = config["kraken_level"]

(job, part) = glob_wildcards('output/clustering/{JOBID}_diffs{PART}.csv')

rule all:
    input:
      expand("output/clustering/{JOBID}_output_{PART}.csv", JOBID = JOBID, PART = part),
      expand("output/clustering/{JOBID}_parallel_sets_{PART}.csv", JOBID = JOBID, PART = part),
      expand("output/clustering/{JOBID}_parallel_merged.out", JOBID = JOBID),
      expand("output/clustering/{JOBID}_non_red_list.out", JOBID = JOBID),
      expand("logs/{JOBID}_para_done.txt", JOBID=JOBID),
      expand("logs/{JOBID}_checkm_unbinned", JOBID=JOBID),
      expand("output/results/{JOBID}_singleton.fa", JOBID=JOBID),
      expand("logs/{JOBID}_singleton_step1.txt", JOBID=JOBID)


rule bin_feeder:
    input:
        diffs = 'output/clustering/' + JOBID + '_diffs{PART}.csv'
    output:
        all = "output/clustering/" + JOBID + "_output_{PART}.csv",
    params:
        thresh = P_THRESH,
        all_diffs = expand("output/clustering/{JOBID}_diffs.csv", JOBID = JOBID)
    conda:
        "../envs/py3.yaml"
    shell:
        """
        python scripts/bin_feeder.py {input.diffs} {params.all_diffs} {params.thresh} {output.all}
        """

rule para_sets:
    input:
        bins = "output/clustering/" + JOBID + "_output_{PART}.csv"
    output:
        "output/clustering/" + JOBID + "_parallel_sets_{PART}.csv"
    params:
        thresh = P_THRESH
    conda:
        "../envs/py3.yaml"
    shell:
        """
        python scripts/para_sets.py {input.bins} {output} {params.thresh}
        """

rule para_merge:
    input:
        expand("output/clustering/{JOBID}_parallel_sets_{PART}.csv", JOBID=JOBID, PART = part)
    output:
        "output/clustering/{JOBID}_parallel_merged.out"
    resources:
        mem_mb = 64000
    conda:
        "../envs/py3.yaml"
    shell:
        """
        python scripts/parallel_merge_step2.py -i {input} -o {output}
        """

rule non_red_step:
    input:
      expand("output/clustering/{JOBID}_parallel_merged.out", JOBID = JOBID)
    output:
      expand("output/clustering/{JOBID}_non_red_list.out", JOBID = JOBID)
    conda:
      "../envs/py3.yaml"
    shell:
      """
      python scripts/step3.py {input} {output}
      """

rule file_parser:
    input:
        expand("output/clustering/{JOBID}_non_red_list.out", JOBID = JOBID)
    output:
        touch("logs/{JOBID}_para_done.txt")
    params:
        contigs = REFIN,
        csv = expand("output/clustering/{JOBID}_read_counts_derived.csv", JOBID = JOBID),
        wd = "results/",
        header = samples
    conda:
        "../envs/py3.yaml"
    shell:
        """
        mkdir -p output/{params.wd}
        python scripts/file_parser.py {params.contigs} {params.csv} {input} {params.wd} -l {params.header}
        """

rule checkm_unbin:
    input:
        expand("logs/{JOBID}_para_done.txt", JOBID = JOBID)
    output:
        touch("logs/{JOBID}_checkm_unbinned")
    params:
        input = "output/results",
        refin = REFIN,
        checkmdb = config["checkm_db_root"]
    threads:
        20
    conda:
        "../envs/checkm.yaml"
    shell:
        """
	      checkm_db={params.checkmdb}
        echo ${{checkm_db}} | checkm data setRoot ${{checkm_db}}
        checkm unbinned -x fasta output/results/ {params.refin} output/results/{JOBID}_unbinned_contigs.fa output/results/{JOBID}_unbinned_contigs_stats.tsv
        """

rule singleton:
    input:
        expand("logs/{JOBID}_checkm_unbinned", JOBID = JOBID)
    output:
        expand("output/results/{JOBID}_singleton.fa", JOBID=JOBID)
    params:
        min_size = 500000
    conda:
        "../envs/py3.yaml"
    shell:
        """
        seqkit seq -m {params.min_size} output/results/*unbinned_contigs.fa > output/results/{JOBID}_singleton.fa
        """

rule singleton_split:
    input:
        "output/results/{JOBID}_singleton.fa"
    output:
        touch("logs/{JOBID}_singleton_step1.txt")
    shell:
        """
        count=0
        while read line
        do
            if [[ ${{line:0:1}} == '>' ]]
            then
                outfile=output/results/Cluster_singleton${{count}}.fasta
                echo ${{line}} > ${{outfile}}
                count=$((count+1))
            else
                echo ${{line}} >> ${{outfile}}
            fi
        done < {input}
        """
